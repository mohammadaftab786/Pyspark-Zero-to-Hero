{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8db0429-794d-48a9-a367-2e8d05976be2",
   "metadata": {},
   "source": [
    "# Spark Salting technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39beda94-3afa-4a28-b011-22af1528c518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://e85ba8ba0610:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Salting technique</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f1ce40a7a00>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Spark Session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Salting technique\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f2db4a8-85f3-4f82-a64a-067f865d04a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create the example dataset of fact and dimesion we would use for demonstration\n",
    "# Python program to generate random Fact table data\n",
    "# [1, ,\"ORD1001\", \"D102\", 56]\n",
    "import random\n",
    "\n",
    "\n",
    "def generate_fact_data(counter=100):\n",
    "    fact_records = []\n",
    "    dim_keys = [\"D100\", \"D101\", \"D102\", \"D103\", \"D104\"]\n",
    "    order_ids = [\"ORD\" + str(i) for i in range(1001, 1010)]\n",
    "    qty_range = [i for i in range(10, 120)]\n",
    "    for i in range(counter):\n",
    "        _record = [i, random.choice(order_ids), random.choice(dim_keys), random.choice(qty_range)]\n",
    "        fact_records.append(_record)\n",
    "    return fact_records\n",
    "\n",
    "# We will generate 200 records with random data in Fact to create skewness\n",
    "fact_records = generate_fact_data(200)\n",
    "\n",
    "dim_records = [\n",
    "    [\"D100\", \"Product A\"],\n",
    "    [\"D101\", \"Product B\"],\n",
    "    [\"D102\", \"Product C\"],\n",
    "    [\"D103\", \"Product D\"],\n",
    "    [\"D104\", \"Product E\"]\n",
    "]\n",
    "\n",
    "_fact_cols = [\"id\", \"order_id\", \"prod_id\", \"qty\"]s\n",
    "_dim_cols = [\"prod_id\", \"prod_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7ffb3da-39e7-42f1-83ef-284bb096bca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- prod_id: string (nullable = true)\n",
      " |-- qty: long (nullable = true)\n",
      "\n",
      "+---+--------+-------+---+\n",
      "|id |order_id|prod_id|qty|\n",
      "+---+--------+-------+---+\n",
      "|0  |ORD1009 |D101   |50 |\n",
      "|1  |ORD1007 |D102   |55 |\n",
      "|2  |ORD1007 |D103   |87 |\n",
      "|3  |ORD1007 |D103   |23 |\n",
      "|4  |ORD1007 |D103   |15 |\n",
      "|5  |ORD1004 |D100   |20 |\n",
      "|6  |ORD1009 |D103   |57 |\n",
      "|7  |ORD1002 |D100   |51 |\n",
      "|8  |ORD1008 |D101   |61 |\n",
      "|9  |ORD1004 |D101   |74 |\n",
      "+---+--------+-------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate Fact Data Frame\n",
    "fact_df = spark.createDataFrame(data = fact_records, schema=_fact_cols)\n",
    "\n",
    "fact_df.printSchema()\n",
    "fact_df.show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebbebb19-841f-48fe-8cad-277c93e01d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- prod_id: string (nullable = true)\n",
      " |-- prod_name: string (nullable = true)\n",
      "\n",
      "+-------+---------+\n",
      "|prod_id|prod_name|\n",
      "+-------+---------+\n",
      "|D100   |Product A|\n",
      "|D101   |Product B|\n",
      "|D102   |Product C|\n",
      "|D103   |Product D|\n",
      "|D104   |Product E|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate Prod Dim Data Frame\n",
    "dim_df = spark.createDataFrame(data = dim_records, schema=_dim_cols)\n",
    "\n",
    "dim_df.printSchema()\n",
    "dim_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5c991eb7-a626-4d9c-b0c9-b56b69d6a4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Set Spark parameters - We have to turn off AQL to demonstrate Salting\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n",
    "\n",
    "# Check the parameters\n",
    "print(spark.conf.get(\"spark.sql.adaptive.enabled\"))\n",
    "print(spark.conf.get(\"spark.sql.shuffle.partitions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "628d0d86-000b-46ba-8bcb-852da95d9f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------+---+---------+\n",
      "|prod_id|id |order_id|qty|prod_name|\n",
      "+-------+---+--------+---+---------+\n",
      "|D103   |2  |ORD1007 |87 |Product D|\n",
      "|D103   |3  |ORD1007 |23 |Product D|\n",
      "|D103   |4  |ORD1007 |15 |Product D|\n",
      "|D103   |6  |ORD1009 |57 |Product D|\n",
      "|D103   |10 |ORD1009 |99 |Product D|\n",
      "|D103   |26 |ORD1008 |66 |Product D|\n",
      "|D103   |34 |ORD1004 |63 |Product D|\n",
      "|D103   |53 |ORD1006 |102|Product D|\n",
      "|D103   |55 |ORD1002 |90 |Product D|\n",
      "|D103   |59 |ORD1009 |19 |Product D|\n",
      "+-------+---+--------+---+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets join the fact and dim without salting\n",
    "joined_df = fact_df.join(dim_df, on=\"prod_id\", how=\"leftouter\")\n",
    "joined_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a0e19774-3eef-4f89-964a-daa413ed4f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|partition_num|count(id)|\n",
      "+-------------+---------+\n",
      "|            4|      119|\n",
      "|            2|       81|\n",
      "+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the partition details to understand distribution\n",
    "from pyspark.sql.functions import spark_partition_id, count\n",
    "\n",
    "partition_df = joined_df.withColumn(\"partition_num\", spark_partition_id()).groupBy(\"partition_num\").agg(count(\"id\"))\n",
    "partition_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bfb6cbe8-2ef3-4c57-8f3d-83ff44b36e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let prepare the salt\n",
    "import random\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# UDF to return a random number every time\n",
    "def rand(): return random.randint(0, 4) #Since we are distributing the data in 5 partitions\n",
    "rand_udf = udf(rand)\n",
    "\n",
    "# Salt Data Frame to add to dimension\n",
    "salt_df = spark.range(0, 5)\n",
    "salt_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f13fb4db-1d68-492a-99c7-1d0532fc5e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+---+--------------+\n",
      "|id |order_id|prod_id|qty|salted_prod_id|\n",
      "+---+--------+-------+---+--------------+\n",
      "|0  |ORD1009 |D101   |50 |D101_4        |\n",
      "|1  |ORD1007 |D102   |55 |D102_4        |\n",
      "|2  |ORD1007 |D103   |87 |D103_0        |\n",
      "|3  |ORD1007 |D103   |23 |D103_2        |\n",
      "|4  |ORD1007 |D103   |15 |D103_4        |\n",
      "|5  |ORD1004 |D100   |20 |D100_0        |\n",
      "|6  |ORD1009 |D103   |57 |D103_2        |\n",
      "|7  |ORD1002 |D100   |51 |D100_3        |\n",
      "|8  |ORD1008 |D101   |61 |D101_2        |\n",
      "|9  |ORD1004 |D101   |74 |D101_1        |\n",
      "+---+--------+-------+---+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Salted Fact\n",
    "from pyspark.sql.functions import lit, expr, concat\n",
    "\n",
    "salted_fact_df = fact_df.withColumn(\"salted_prod_id\", concat(\"prod_id\",lit(\"_\"), lit(rand_udf())))\n",
    "salted_fact_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6fb4c87e-b86a-4d11-b3df-dd21c8395394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------+\n",
      "|prod_id|prod_name|salted_prod_id|\n",
      "+-------+---------+--------------+\n",
      "|   D100|Product A|        D100_0|\n",
      "|   D100|Product A|        D100_1|\n",
      "|   D100|Product A|        D100_2|\n",
      "|   D100|Product A|        D100_3|\n",
      "|   D100|Product A|        D100_4|\n",
      "|   D101|Product B|        D101_0|\n",
      "|   D101|Product B|        D101_1|\n",
      "|   D101|Product B|        D101_2|\n",
      "|   D101|Product B|        D101_3|\n",
      "|   D101|Product B|        D101_4|\n",
      "|   D102|Product C|        D102_0|\n",
      "|   D102|Product C|        D102_1|\n",
      "|   D102|Product C|        D102_2|\n",
      "|   D102|Product C|        D102_3|\n",
      "|   D102|Product C|        D102_4|\n",
      "|   D103|Product D|        D103_0|\n",
      "|   D103|Product D|        D103_1|\n",
      "|   D103|Product D|        D103_2|\n",
      "|   D103|Product D|        D103_3|\n",
      "|   D103|Product D|        D103_4|\n",
      "+-------+---------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Salted DIM\n",
    "salted_dim_df = dim_df.join(salt_df, how=\"cross\").withColumn(\"salted_prod_id\", concat(\"prod_id\", lit(\"_\"), \"id\")).drop(\"id\")\n",
    "\n",
    "salted_dim_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "221c5156-e61d-4199-8455-f107e3c4ee5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+--------+-------+---+-------+---------+\n",
      "|salted_prod_id|id |order_id|prod_id|qty|prod_id|prod_name|\n",
      "+--------------+---+--------+-------+---+-------+---------+\n",
      "|D100_0        |126|ORD1006 |D100   |44 |D100   |Product A|\n",
      "|D100_0        |185|ORD1004 |D100   |20 |D100   |Product A|\n",
      "|D100_1        |106|ORD1008 |D100   |106|D100   |Product A|\n",
      "|D100_1        |175|ORD1005 |D100   |54 |D100   |Product A|\n",
      "|D104_1        |50 |ORD1002 |D104   |101|D104   |Product E|\n",
      "|D104_1        |81 |ORD1006 |D104   |32 |D104   |Product E|\n",
      "|D104_1        |177|ORD1002 |D104   |23 |D104   |Product E|\n",
      "|D104_1        |178|ORD1007 |D104   |11 |D104   |Product E|\n",
      "|D101_1        |103|ORD1007 |D101   |115|D101   |Product B|\n",
      "|D103_1        |2  |ORD1007 |D103   |87 |D103   |Product D|\n",
      "+--------------+---+--------+-------+---+-------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets make the salted join now\n",
    "salted_joined_df = salted_fact_df.join(salted_dim_df, on=\"salted_prod_id\", how=\"leftouter\")\n",
    "salted_joined_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bc77f98c-ddd8-495a-8a5f-9d80eb2e6224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|partition_num|count|\n",
      "+-------------+-----+\n",
      "|            0|   18|\n",
      "|            1|   64|\n",
      "|            2|   24|\n",
      "|            3|   61|\n",
      "|            4|   33|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the partition details to understand distribution\n",
    "from pyspark.sql.functions import spark_partition_id, count\n",
    "\n",
    "partition_df = salted_joined_df.withColumn(\"partition_num\", spark_partition_id()).groupBy(\"partition_num\") \\\n",
    "    .agg(count(lit(1)).alias(\"count\")).orderBy(\"partition_num\")\n",
    "\n",
    "partition_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f749ff1e-2abc-4d15-b789-9257c4960335",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
