{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e662cf22-28a0-4533-b2b8-f8e1715fcada",
   "metadata": {},
   "source": [
    "# COUNT(1) vs COUNT(*) vs COUNT(COL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "526666cf-8ac8-409f-89dd-512a6be00011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://e85ba8ba0610:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Count(1) vs Count(*)</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f04a2d584f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Spark Session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Count(1) vs Count(*)\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08a916c0-98fa-4987-b979-42946cca6283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a simple Python decorator - {get_time} to get the execution timings\n",
    "# If you dont know about Python decorators - check out : https://www.geeksforgeeks.org/decorators-in-python/\n",
    "import time\n",
    "\n",
    "def get_time(func):\n",
    "    def inner_get_time() -> str:\n",
    "        start_time = time.time()\n",
    "        func()\n",
    "        end_time = time.time()\n",
    "        return (f\"Execution time: {(end_time - start_time)*1000} ms\")\n",
    "    print(inner_get_time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "69801eee-aec0-47f9-96f4-6c2ccf671a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----------+--------------------+--------------------+----------+\n",
      "|       transacted_at|    trx_id|retailer_id|         description|              amount|   city_id|\n",
      "+--------------------+----------+-----------+--------------------+--------------------+----------+\n",
      "|2017-11-24T19:00:...|1995601912| 2077350195|Walgreen       11-25|197.2300000000000...| 216510442|\n",
      "|2017-11-24T19:00:...|1734117021|  644879053|unkn    ppd id: 7...|8.580000000000000000| 930259917|\n",
      "|2017-11-24T19:00:...|1734117022|  847200066|Wal-Mart  ppd id:...|1737.260000000000...|1646415505|\n",
      "|2017-11-24T19:00:...|1734117030| 1953761884|Home Depot     pp...|384.5000000000000...| 287177635|\n",
      "|2017-11-24T19:00:...|1734117089| 1898522855| Target        11-25|66.33000000000000...|1855530529|\n",
      "|2017-11-24T19:00:...|1734117117|  997626433|Sears  ppd id: 85...|298.8700000000000...| 957346984|\n",
      "|2017-11-24T19:00:...|1734117123| 1953761884|unkn   ppd id: 15...|19.55000000000000...|  45522086|\n",
      "|2017-11-24T19:00:...|1734117152| 1429095612|Ikea     arc id: ...|9.390000000000000000|1268541279|\n",
      "|2017-11-24T19:00:...|1734117153|  847200066|unkn        Kings...|2907.570000000000...|1483931123|\n",
      "|2017-11-24T19:00:...|1734117212| 1996661856|unkn    ppd id: 4...|140.3800000000000...| 336763936|\n",
      "|2017-11-24T19:00:...|1734117241|  486576507|              iTunes|2912.670000000000...|1663872965|\n",
      "|2017-11-24T19:00:...|2076947148|  847200066|Wal-Mart         ...|62.83000000000000...|1556600840|\n",
      "|2017-11-24T19:00:...|2076947147|  562903918|McDonald's    ccd...|31.37000000000000...| 930259917|\n",
      "|2017-11-24T19:00:...|2076947146|  511877722|unkn     ccd id: ...|1915.350000000000...|1698762556|\n",
      "|2017-11-24T19:00:...|2076947113| 1996661856|AutoZone  arc id:...|1523.600000000000...|1759612211|\n",
      "|2017-11-24T19:00:...|2076947018|  902350112|DineEquity    arc...|22.28000000000000...|2130657559|\n",
      "|2017-11-24T19:00:...|2076946994| 1898522855|Target    ppd id:...|2589.930000000000...|2074005445|\n",
      "|2017-11-24T19:00:...|2076946985|  847200066|Wal-Mart    ppd i...|42.20000000000000...| 459344513|\n",
      "|2017-11-24T19:00:...|2076946960|  386167994|Wendy's  ppd id: ...|14.62000000000000...| 352952442|\n",
      "|2017-11-24T19:00:...|2076946954|  486576507|iTunes     ppd id...|37.42000000000000...| 485114748|\n",
      "+--------------------+----------+-----------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets read the dataframe to check the data\n",
    "df = spark \\\n",
    "    .read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .load(\"dataset/sales.csv\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10464096-da89-498a-be92-45808a495330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 2384.6654891967773 ms\n"
     ]
    }
   ],
   "source": [
    "# Get count(1) performance\n",
    "from pyspark.sql.functions import lit, count\n",
    "\n",
    "@get_time\n",
    "def x(): df.groupBy(\"trx_id\").agg(count(lit(1))).write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "290e8312-4fe3-4f90-8725-005b25b88a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 2509.0878009796143 ms\n"
     ]
    }
   ],
   "source": [
    "# Get count(col_name) performance\n",
    "@get_time\n",
    "def x(): df.groupBy(\"trx_id\").agg(count(\"city_id\")).write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "863ac0ef-7829-42b6-90a4-2f83d1b48dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 2393.6233520507812 ms\n"
     ]
    }
   ],
   "source": [
    "# Get count(*) performance\n",
    "@get_time\n",
    "def x(): df.groupBy(\"trx_id\").agg(count(\"*\")).write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6bd2a5ed-f3e2-4c84-98f2-8155ce9ec0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['trx_id], ['trx_id, count(1) AS count(1)#512L]\n",
      "+- Relation [transacted_at#17,trx_id#18,retailer_id#19,description#20,amount#21,city_id#22] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "trx_id: string, count(1): bigint\n",
      "Aggregate [trx_id#18], [trx_id#18, count(1) AS count(1)#512L]\n",
      "+- Relation [transacted_at#17,trx_id#18,retailer_id#19,description#20,amount#21,city_id#22] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [trx_id#18], [trx_id#18, count(1) AS count(1)#512L]\n",
      "+- Project [trx_id#18]\n",
      "   +- Relation [transacted_at#17,trx_id#18,retailer_id#19,description#20,amount#21,city_id#22] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[trx_id#18], functions=[count(1)], output=[trx_id#18, count(1)#512L])\n",
      "   +- Exchange hashpartitioning(trx_id#18, 200), ENSURE_REQUIREMENTS, [id=#992]\n",
      "      +- HashAggregate(keys=[trx_id#18], functions=[partial_count(1)], output=[trx_id#18, count#516L])\n",
      "         +- FileScan csv [trx_id#18] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/EaseWithApacheSpark/ease-with-apache-spark/dataset/s..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<trx_id:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explain Plan for count(*)\n",
    "df.groupBy(\"trx_id\").agg(count(\"*\")).explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9695b587-7a54-4ee8-b5e4-f75fbe5493c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['trx_id], ['trx_id, count(1) AS count(1)#500L]\n",
      "+- Relation [transacted_at#17,trx_id#18,retailer_id#19,description#20,amount#21,city_id#22] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "trx_id: string, count(1): bigint\n",
      "Aggregate [trx_id#18], [trx_id#18, count(1) AS count(1)#500L]\n",
      "+- Relation [transacted_at#17,trx_id#18,retailer_id#19,description#20,amount#21,city_id#22] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [trx_id#18], [trx_id#18, count(1) AS count(1)#500L]\n",
      "+- Project [trx_id#18]\n",
      "   +- Relation [transacted_at#17,trx_id#18,retailer_id#19,description#20,amount#21,city_id#22] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[trx_id#18], functions=[count(1)], output=[trx_id#18, count(1)#500L])\n",
      "   +- Exchange hashpartitioning(trx_id#18, 200), ENSURE_REQUIREMENTS, [id=#979]\n",
      "      +- HashAggregate(keys=[trx_id#18], functions=[partial_count(1)], output=[trx_id#18, count#504L])\n",
      "         +- FileScan csv [trx_id#18] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/EaseWithApacheSpark/ease-with-apache-spark/dataset/s..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<trx_id:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explain Plan for count(1)\n",
    "df.groupBy(\"trx_id\").agg(count(lit(1))).explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d6fb2fd0-bccb-4bc4-9224-87020e87b8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['trx_id], ['trx_id, count('city_id) AS count(city_id)#488]\n",
      "+- Relation [transacted_at#17,trx_id#18,retailer_id#19,description#20,amount#21,city_id#22] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "trx_id: string, count(city_id): bigint\n",
      "Aggregate [trx_id#18], [trx_id#18, count(city_id#22) AS count(city_id)#488L]\n",
      "+- Relation [transacted_at#17,trx_id#18,retailer_id#19,description#20,amount#21,city_id#22] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [trx_id#18], [trx_id#18, count(city_id#22) AS count(city_id)#488L]\n",
      "+- Project [trx_id#18, city_id#22]\n",
      "   +- Relation [transacted_at#17,trx_id#18,retailer_id#19,description#20,amount#21,city_id#22] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[trx_id#18], functions=[count(city_id#22)], output=[trx_id#18, count(city_id)#488L])\n",
      "   +- Exchange hashpartitioning(trx_id#18, 200), ENSURE_REQUIREMENTS, [id=#966]\n",
      "      +- HashAggregate(keys=[trx_id#18], functions=[partial_count(city_id#22)], output=[trx_id#18, count#492L])\n",
      "         +- FileScan csv [trx_id#18,city_id#22] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/EaseWithApacheSpark/ease-with-apache-spark/dataset/s..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<trx_id:string,city_id:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explain plan with count(col_name)\n",
    "df.groupBy(\"trx_id\").agg(count(\"city_id\")).explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2336c58b-32f3-4ada-bed7-db834417f83c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
